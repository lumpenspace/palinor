{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lumpenspace/palinor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and set up the repository\n",
    "%cd palinor\n",
    "\n",
    "# Install poetry and dependencies\n",
    "!curl -sSL https://install.python-poetry.org | python3 -\n",
    "!poetry install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from palinor import create_dataset\n",
    "from palinor.ControllableModel import ControllableModel\n",
    "from palinor.ControlVector import ControlVector\n",
    "from rich.console import Console\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Initialize console and login\n",
    "console = Console()\n",
    "HF_TOKEN = \"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\"\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Create cache directories\n",
    "cache_dir = Path(\"/workspace/.cache/huggingface\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "os.environ['HF_HOME'] = str(cache_dir)  # Set environment variable for HF cache\n",
    "\n",
    "# Set model name and device\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "console.print(f\"Using device: {device}\")\n",
    "\n",
    "# Memory check before loading\n",
    "if torch.cuda.is_available():\n",
    "    console.print(f\"CUDA Memory before loading: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "# Load model with CUDA settings\n",
    "console.print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    cache_dir=str(cache_dir),\n",
    "    local_files_only=False\n",
    ")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "    console.print(\"Model moved to GPU\")\n",
    "    console.print(f\"CUDA Memory after loading: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "# Load tokenizer\n",
    "console.print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=HF_TOKEN,\n",
    "    padding_side=\"left\",\n",
    "    cache_dir=str(cache_dir)\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create controllable model\n",
    "console.print(\"Creating controllable model...\")\n",
    "controllable_model = ControllableModel(model, layer_ids=[-1, -2, -3])\n",
    "\n",
    "# Create dataset\n",
    "console.print(\"Creating dataset...\")\n",
    "template_path = Path(\"./dataset_templates/vietest.yaml\")\n",
    "if not template_path.exists():\n",
    "    raise FileNotFoundError(f\"Template file not found at {template_path}\")\n",
    "\n",
    "prompts = create_dataset.create_personality_prompts(\n",
    "    str(template_path),\n",
    "    a_adjective=\"i am sad, and i guess ill respond in all lowercase...\",\n",
    "    b_adjective=\"I HAVE TOO MUCH ENERGY AND LOVE TO RESPOND IN ALL CAPS!\"\n",
    "\n",
    ")\n",
    "create_dataset.save_prompts(prompts, \"vector_dataset.jsonl\")\n",
    "console.print(f\"Created dataset with {len(prompts)} prompts\")\n",
    "\n",
    "# Train vector\n",
    "console.print(\"Training control vector...\")\n",
    "try:\n",
    "    control_vector = ControlVector.train(\n",
    "        model=controllable_model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=prompts,\n",
    "        max_batch_size=4 if device == \"cpu\" else 32\n",
    "    )\n",
    "    \n",
    "    # Save vector\n",
    "    vector_path = Path(\"/workspace/vectors\")\n",
    "    vector_path.mkdir(exist_ok=True)\n",
    "    control_vector.to_file(str(vector_path / \"a_b_vector.pkl\"))\n",
    "    console.print(\"[green]Control vector trained and saved![/green]\")\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[red]Error during training: {str(e)}[/red]\")\n",
    "    raise\n",
    "\n",
    "# Test vector generation\n",
    "console.print(\"\\nTesting vector generation...\")\n",
    "test_prompt = \"Me? What do I care about?\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "try:\n",
    "    for i in range(10): #run this a few times\n",
    "        # Generate with control valence pole A\n",
    "        controllable_model.set_control(control_vector, coeff=10.0)\n",
    "        with torch.inference_mode():\n",
    "            low_output = tokenizer.decode(\n",
    "                controllable_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "    \n",
    "        # Generate with control valence pole B\n",
    "        controllable_model.set_control(control_vector, coeff=-10.0)\n",
    "        with torch.inference_mode():\n",
    "            high_output = tokenizer.decode(\n",
    "                controllable_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "    \n",
    "        # Print results\n",
    "        console.print(\"\\n[bold]Test outputs:[/bold]\")\n",
    "        console.print(\"[blue]A:[/blue]\", low_output)\n",
    "        console.print(\"[purple]B:[/purple]\", high_output)\n",
    "\n",
    "        #track where we are\n",
    "        print(f\"...this was iteration {i +1}\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[red]Error during generation: {str(e)}[/red]\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Clean up\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        console.print(\"\\nCUDA memory cleared\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
